\chapter*{Заключение}

В данной работе были проведены эксперименты с исследованием электронных писем Хиллари Клинтон и корпорации $Enron$. 

Основная проблема в исследовании писем Клинтон --- недостаточно большой размер датасета. Это приводит к проблеме недостаточного уровня обученности моделей. Она решается с помощью 
моделей, предобученных на датасетах большего размера. Здесь и тематическое моделирование, и кластеризация показали неплохие интерпретируемые результаты, о чем можно судить по представленным таблицам в соответствующих разделах работы.

Что касается набора $Enron$, он содержит гораздо больше данных, однако содержимое писем представлено не очень качественно (например, много автоматически генерируемого текста, полученного сервисами
для отправки писем, который не несёт никакой смысловой информации), поэтому была проведена более серьёзная работа по очистке текста. Как и в случае
набора данных Клинтон, модели тематического моделирования и кластеразация векторных 
представлений слов показали хорошие интерпретируемые результаты. Для набора $Enron$ также использовалась техника кластеризации слов, основанная на векторных представлениях целых предложений, а не отдельных слов --- такой метод показал ещё более интерпретируемые результаты. Также для писем $Enron$ был применен алгоритм $BERTopic$, который сгенерировал хорошо интерпретируемые темы, но, в сравненении с моделью Латентного размещения Дирихле, тем получилось намного больше.

На обоих датасетах был произведен тщательный и внимательный анализ писем и их метаданных, построено большое количество визуализаций и распределений различной информации о письмах.

Для лучшей работы всех используемых моделей была произведена оптимизация гиперпараметров и техника передачи обучения (англ. \textit{Transfer Learning}), позволяющая использовать предобученные модели, которые <<видели>> большое количество данных при обучении и были впоследствии дообучены на исходных электронных переписках. С использованием этой техники результаты получились лучше, чем при обучении модели <<с нуля>>, то есть в случае, когда все параметры модели инициализируются константами или же случайным образом.

Если сравнивать результаты с результатами работ пользователей $Kaggle$, исследование данных проведено более тщательно и результаты данной работы получились более интуитивными и интерпретируемыми.

Исходный код работы доступен по ссылке: \url{https://github.com/kefaa/clinton-emails-research}.

\addcontentsline{toc}{chapter}{Заключение}
\clearpage
