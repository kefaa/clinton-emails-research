
\section{Предобработка электронных писем Хиллари Клинтон}

В отличие от набора данных корпорации $Enron$, набор данных Клинтон
 не содержит метаданных письма --- всё уже разделено на дату отправки письма, 
 информацию о получателях и так далее, поэтому в этом случае нам будет немного
 проще.

Посмотрим на пример содержания письма, до его обработки:
\begin{verbatim}
 Nice\nForgot to tell you about our harrowing circling 
 and attempted landings last night ...
\end{verbatim}


Этап предварительном обработки данных можно разбить на 7 шагов:

\begin{enumerate}

\item
Для начала нужно содержание каждого документа (он может быть формата $docx$, $pdf$ и так далее) представить в виде последовательности слов (или нескольких последовательностей), чтобы далее можно было применять методы машинного обучения. Этот шаг уже произведен платформой \textit{Kaggle}. Общее количество электронных писем --- 7945.

\item Может оказаться, что исходные документы были плохого качества (например, <<битые>> 
\textit{pdf}-файлы, или же версия документа давно уже не поддерживается). Поэтому в данных
у нас могут встречаться пропуски. Такие данные мы просто удаляем из датасета. После осуществления этого шага остается 6742 писем. 

\item Следующим шагом идёт нормализация, то есть удаляются знаки препинания, подряд идущие пробелы, переносы строк, выделяются конкретные слова. Это помогает уменьшить количество различной информации, с которой приходится иметь дело компьютеру, и, следовательно, повышает
качество и эффективность алгоритмов. Для этого была использована 
библиотека $nltk$ \cite{nltkk}, 
содердащая большое количество инструментов для обработки текстов.

\item Далее каждые слово приводится к нижнему регистру. Это делается для чтобы, например,
слова $Hello$ и $hello$ модель воспринимала как одно слово, ведь по смыслу они означают
одно и то же.

\item Потом удаляются стоп-слова --- слова в языке, которые имеют малое смысловое значение (местоимения, предлоги и так далее). Они встречаются в изобилии, поэтому практически не предоставляют уникальной информации, которая может быть использована для классификации или кластеризации данных. 

\item Следующий шаг --- лемматизация. При обработке естественного языка может наступить момент, когда вы захотите, чтобы программа распознала, что слова $ask$ и $asked$ --- это просто разные времена одного и того же глагола. Это идея сведения различных форм слова к основному корню. Такая процедура заметно уменьшает количество уникальных слов в тексте, что
является большим плюсом, также модель будет понимать, что это одно и то же слово, что 
заметно повышает качество работы многих моделей машинного обучения. Для реализации такой процедуры была использовано библиотека \textit{spaCy} \cite{bib3}. На данный момент почти
всё готово к тому, чтобы давать полученные данные на вход модели.

\item Последним шагом является индексация слов. Нужно слова привести в формат, понятный 
компьютеру, и над полученными представлениями проводить некоторые арифметические операции. Для этого каждому слову достаточно сопоставить уникальный числовой идентификатор.
 
\end{enumerate}

Посмотрим на это же письмо после обработки (не включая этап индексации):
\begin{verbatim}
 nice forget tell harrow circle attempt landing last night
\end{verbatim}

Видно, что все этапы прошли успешно. Теперь всё готово к тому, чтобы данные передавать на вход модели.

