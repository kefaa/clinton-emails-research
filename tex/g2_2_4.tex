\subsection{Алгоритм}

\begin{itemize}
\item Взять каждый документ и случайным образом определить каждому слову в документе одну из $k$ тем (напомним, что $k$ выбирается заранее).

\item Для каждого документа $d$ взять каждое слово $w$ и вычислить:
\begin{enumerate}
\item $p(\text{тема } t \text{ } | \text{ }  \text{документ } d)$ --- долю слов в документе $d$, которые относятся к теме $t$. С помощью этого значения производится попытка определить, сколько слов принадлежит теме $t$ для данного документа $d$, исключая текущее слово $w$. Чем больше слов из $d$ принадлежит $t$, тем более вероятно, что слово $w$ принадлежит t.
\item $p(\text{слово } w \text{ }  | \text{ }  \text{тема } t)$ --- долю распределений темы $t$ по всем документам, которые содержат слово $w$. С помощью этого значения производится попытка определить, сколько документов относятся к теме $t$, опираясь на слово $w$.

\textit{LDA} представляет документы как неупородяченный набор тем. Точно так же тема --- это неупорядоченный слов. Если слово имеет высокую вероятность отношения к теме, все документы, содержащие $w$, также будут более прочно связаны с $t$. Точно так же, если $w$ относится к $t$ с небольшой вероятностью, документы, содержащие $w$, будут иметь очень низкую вероятность отношения к $t$, потому что остальные слова в $d$ будут принадлежать какой-то другой теме и, следовательно, $d$ будет иметь более высокую вероятность по отношению к той теме. Таким образом, даже если $w$ будет добавлено к $t$, оно не принесет много похожих документов к $t$. 
\end{enumerate}

\item Обновить вероятность приналежности слова $w$ теме $t$ следующим образом:
\begin{center}
$p(w \in t) = p(\text{тема } t \text{ }  | \text{ }  \text{документ } d) \cdot p(\text{слово } w \text{ }  | \text{ }  \text{тема } t)$
\end{center}
\end{itemize}